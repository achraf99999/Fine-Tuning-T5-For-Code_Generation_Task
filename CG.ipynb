{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97Av32IAvNOU",
    "outputId": "e6cdb10a-df4f-4e3d-d8a2-4b6275b47e13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ExFzbWxoC-oN",
    "outputId": "b78907c1-1874-430c-ace7-02f5f71a055c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount(\"/content/drive\", force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "v8IdHyYYzK2i"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hulJwaYT8Jd"
   },
   "source": [
    "# **Install dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dIVCID_z9pMw"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!! pip install keras\n",
    "!! pip install netron\n",
    "!! pip install nltk\n",
    "\n",
    "!! pip install git+https://github.com/huggingface/transformers\n",
    "!! pip install datasets\n",
    "!! pip install nlp\n",
    "!! pip install rouge-score\n",
    "\n",
    "!! pip install git-python\n",
    "!! pip install sentencepiece\n",
    "\n",
    "!! pip install sacrebleu==1.5.1\n",
    "!! pip install tree-sitter\n",
    "!! pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!! git clone https://github.com/huggingface/evaluate.git\n",
    "%cd evaluate\n",
    "!! pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuI8u_wO1_um",
    "outputId": "301db97c-e746-422a-d3ff-10649f083c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchdata\n",
      "  Downloading torchdata-0.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.32.3)\n",
      "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.0+cu121)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2->torchdata)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n",
      "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchdata\n",
      "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchdata-0.7.1\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.43.0.dev0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (1.5.1)\n",
      "Requirement already satisfied: portalocker==2.0.0 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.0.0)\n"
     ]
    }
   ],
   "source": [
    "!! pip install torchdata\n",
    "!! pip install transformers\n",
    "!! pip install sacrebleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e9_D0csUCTl"
   },
   "source": [
    "# **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "m4IWUUZYvcBH",
    "outputId": "c26f53ca-9a8c-46d4-ef12-bce047c1822c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0543b325601f>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2Tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGPT2TokenizerFast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEncoderDecoderModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "import logging\n",
    "from transformers import BertTokenizer, GPT2Tokenizer, GPT2TokenizerFast, EncoderDecoderModel, Trainer, TrainingArguments, BertTokenizerFast\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "import types\n",
    "import argparse\n",
    "import logging\n",
    "from functools import partial\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    BertGenerationConfig,\n",
    "    BertGenerationEncoder,\n",
    "    BertTokenizer,\n",
    "    EncoderDecoderModel,\n",
    "    EncoderDecoderConfig,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2TokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_cosine_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "import sacrebleu\n",
    "import random\n",
    "import numpy as np\n",
    "#from tensorboardX import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fhYGg8K_TzYl"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2YE9oRS-YmTp",
    "outputId": "7487e46b-53d9-4eb1-a00d-e7a62dd99d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian'\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cy34HZRCduwK"
   },
   "source": [
    "# **Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kzhiW57L7bDd"
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "izdwQeiXETS6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, DatasetDict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zPApGdsZDyTd",
    "outputId": "45b4f77a-7551-4ccb-e7e9-085dea6bfec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved successfully at ./dataset.json!\n",
      "Dataset loaded successfully from ./dataset.json!\n"
     ]
    }
   ],
   "source": [
    "# Load the Excel file\n",
    "df = pd.read_excel(\"Internership_TestCase.xlsx\")\n",
    "\n",
    "# Split data into train (80%), validation (10%), and test (10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Convert pandas DataFrames to lists of dictionaries\n",
    "train_data = train_df.to_dict(orient='records')\n",
    "val_data = val_df.to_dict(orient='records')\n",
    "test_data = test_df.to_dict(orient='records')\n",
    "\n",
    "# Create a dictionary with the splits\n",
    "dataset_dict = {\n",
    "    'train': train_data,\n",
    "    'validation': val_data,\n",
    "    'test': test_data\n",
    "}\n",
    "\n",
    "# Save the dictionary to a JSON file\n",
    "save_path = \"./dataset.json\"\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(dataset_dict, f)\n",
    "\n",
    "print(f\"Dataset saved successfully at {save_path}!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GgHYkCaXZ_qk"
   },
   "outputs": [],
   "source": [
    "# Load the dictionary from the JSON file\n",
    "with open(\"./dataset.json\", 'r') as f:\n",
    "    dataset_dict = json.load(f)\n",
    "\n",
    "# Convert the lists of dictionaries back to Hugging Face Datasets\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(dataset_dict['train']))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame(dataset_dict['validation']))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame(dataset_dict['test']))\n",
    "\n",
    "# Create a DatasetDict\n",
    "loaded_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "druiwliGDzOt"
   },
   "outputs": [],
   "source": [
    "# Access the datasets\n",
    "train_dataset = loaded_dataset['train']\n",
    "val_dataset = loaded_dataset['validation']\n",
    "test_dataset = loaded_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XS1twihPEcn9",
    "outputId": "6111b89e-070d-406b-fae2-38b4d6b432ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Actions': 'Click on the Favorite List icon',\n",
       " 'Code Groovy': \"WebUI.click(findTestObject('Wave2/XPath', [('path') : '//span[@class=\\\\'x-btn-icon-el x-btn-icon-el-default-small icon icon-favorite favorite-top \\\\']']))\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3uxvvptpExQh",
    "outputId": "89414107-0a97-44b3-c03e-17f1b92b6954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Actions', 'Code Groovy'],\n",
       "        num_rows: 166\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Actions', 'Code Groovy'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Actions', 'Code Groovy'],\n",
       "        num_rows: 21\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKKKmmpHCW62"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400,
     "referenced_widgets": [
      "5af8b7b00b9a45d598eb37886c0dcfe8",
      "f3f1594da1e648208a83f301e1294b40",
      "cfa92722550c40c4adad981f263dedf0",
      "dc94c2c9199444dfa552740e72dbcae5",
      "09a654952538409fa1d8e669045d4057",
      "89a2c7f83bcb4afdb2498be0a95b3aa8",
      "566bd4e830fa404fb1cc613a4610b285",
      "ceae1bd576a8457088a01c0f89d75a05",
      "14aea39ed06b4540b3c4e19321fdb143",
      "13d5ba2a4856419faadc38f2eb9f6a26",
      "dd0dee67fbfa47faa897ab344808fc87",
      "e2958a5a23974fad878c699b664d5e42",
      "85871408bd954366bd203876c789f1ec",
      "77d2047f044145d7a8587926852b3c2f",
      "f352f8ec0d7e4ff0ad2e9673442b31d3",
      "b80b9133ac9841c6bf87031a6c5c5adb",
      "e3be7c15a27b4d1297209bbb5c35dbf7",
      "b5cc25381bfa4e7c9722e334553fffa0",
      "c6ed4640d99b4159aded9f38d5d54223",
      "9ac961f84eee4c898795a0f0681f5451",
      "ce3cac922be741e2bb3700fbdb737cfb",
      "34fc3354312c44f9b93eed319498f2a1",
      "8d5e5ab2dcf6472eb9dc26395f8bdc64",
      "421153cdc9d44147ab2b43d1f9ec4cd3",
      "235035c3d52f4dd0bfce44433f519308",
      "65134a0abb034b31a41f5c9c8cddf233",
      "75cee6cc1f444045bf227545fdaddacf",
      "9e7f60a229a740baaa39af04b118c878",
      "103adab7cccc4dfdaffac04933db7a01",
      "0284945412944d3bb3901feded769c1b",
      "18ef2ec0220c419ea51b49879a0522a3",
      "0d4f8711dba3482fa4aa7bf6c4e0dec3",
      "9b4f6510e65144e5b91f4a59a95008c6",
      "da8e78ccb4614ede950928f5d57835f8",
      "5047b33728a645478f8037e8c01c4fcf",
      "8fb6a63acfe34842b2218b070af450bd",
      "fadc4c8e8d3b46b2b5863f579c326538",
      "0339c2dfbe10438b819e9d3d738a744c",
      "8bc8b76f33f5451d9fd74769cba111f4",
      "b8d94bdeb025478cb6456ee717fc9349",
      "1fd5d91eeb3945f4bf538d5858e41745",
      "955fd950678742a3907e5e65e59fd2b9",
      "bbbb3939329648199334555b96d16043",
      "41040247b4524eeb810b6e58ad5d9e1f",
      "9409df9ac83f440fa0f6b6aad3dd9876",
      "f39c1c3ac41240709307bc6f1585792f",
      "bd9c04a07c3945569774607e902b76a0",
      "3ef72bad1fba426a9e9996d719db27aa",
      "dc815f2b6915436aa66dccd8a01965cd",
      "d3fd2d06408a42c7b841779b03197d6f",
      "973ad2e166c44f1383c86e47bf423c69",
      "a35d287ba2f34078901da04b4669e59f",
      "1085062f824e44ed80b2f35714a56b01",
      "04eaec24117b4323b48d2276298e0369",
      "5e3c35e9e79248f69fc5ecf87099088b",
      "ae2066ba514d424caf1673f460f62068",
      "f059a3ac70e543af87ea70a19a4ce593",
      "3aae563b52334d28a470a4135feeaedd",
      "f228afa8d80f484fb0b5cdc50add038b",
      "76b8f1175b4043a6be744fbe955bcbdd",
      "38dbb389ae3849818d169823dcdf4b94",
      "065bdc3d1e7d4570939a940a759c6f90",
      "545e4ca96cac463ebffc834b0cc9e39b",
      "ea48b58f54054b59b7287adcc87f1f58",
      "c4e25c26834b41918918544ba881bc89",
      "b81a4aef890a42b2b9b324983dd03ad1",
      "3f0065aa04f14fbdb7a9b0c47c3dd9c2",
      "99b99ae0eae3448583131778d153461b",
      "1e122b742b3c4503987042041b812deb",
      "fedc2b818cfe475baa77fc0f0446ba63",
      "c0debc0a85114e5e8a33003e66940abb",
      "548522de6b11407d898ea2fbc41f25ed",
      "0890cd7ca192432caf74e2e9074dc9c5",
      "ec5c336b78fd45e5b1f9ccb266d6e6ae",
      "29f565a53b7b4c639350d442d40bcf5c",
      "679ec853bb454f349d13f5ef8ad9e1ce",
      "a1ae6f0447444e2b97d16cf324c2fd70"
     ]
    },
    "id": "yGq0hRAvsR83",
    "outputId": "a97217cf-3042-4237-bae7-dc83d0579817"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af8b7b00b9a45d598eb37886c0dcfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2958a5a23974fad878c699b664d5e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.91M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d5e5ab2dcf6472eb9dc26395f8bdc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/117k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8e78ccb4614ede950928f5d57835f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9409df9ac83f440fa0f6b6aad3dd9876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2066ba514d424caf1673f460f62068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0065aa04f14fbdb7a9b0c47c3dd9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1805 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_dataset = load_dataset(\"AhmedSSoliman/DJANGO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yNL3RItYwOSl"
   },
   "outputs": [],
   "source": [
    "#train_dataset= raw_dataset[\"train\"]\n",
    "#val_dataset = raw_dataset[\"validation\"]\n",
    "#test_dataset = raw_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6gWJvHZ-RbMP",
    "outputId": "f06613f6-b0f4-4374-eaa4-4c71d7c44fda"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'nl', 'code'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'nl', 'code'],\n",
       "        num_rows: 1805\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'nl', 'code'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8qaEXWphXWk"
   },
   "source": [
    "# **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M2OT3nMFU4Rc",
    "outputId": "b6d80951-5182-4555-d290-54a7674b7235"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='distilroberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilroberta-base\", use_fast = True)\n",
    "tokenizer\n",
    "# \"Helsinki-NLP/opus-mt-en-nl\"\n",
    "# \"distilroberta-base\"\n",
    "##### \"studio-ousia/luke-base\"\n",
    "#\"AhmedSSoliman/MarianCG_NL-to-Code\"\n",
    "#\"distilbert-base-uncased\"\n",
    "#studio-ousia/mluke-base-lite\n",
    "# \"google/electra-small-discriminator\"\n",
    "#\"bert-base-uncased\"\n",
    "#\"microsoft/deberta-v3-xsmall\"\n",
    "#\"microsoft/deberta-v3-base\"\n",
    "# \"xlnet-base-cased\"\n",
    "#\"google/fnet-base\"\n",
    "#\"google/bigbird-roberta-base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07QX2HqSERRj"
   },
   "source": [
    "# **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P5BLbilUN5G4"
   },
   "outputs": [],
   "source": [
    "#All models\n",
    "#model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilroberta-base\", \"Helsinki-NLP/opus-mt-en-nl\")\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilbert-base-uncased\", \"Helsinki-NLP/opus-mt-en-nl\")\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"google/electra-base-discriminator\", \"Helsinki-NLP/opus-mt-en-nl\")\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"studio-ousia/luke-base\", \"Helsinki-NLP/opus-mt-en-nl\")\n",
    "# model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilroberta-base\", \"distilgpt2\")\n",
    "#\"distilgpt2\"\n",
    "#\"studio-ousia/luke-base\"\n",
    "#\"distilroberta-base\"\n",
    "#\"google/electra-base-discriminator\"\n",
    "#\"bert-base-uncased\"\n",
    "#\"xlnet-base-cased\"\n",
    "#\"microsoft/deberta-v3-xsmall\"\n",
    "#\"christina/decoder-only-transformer-x6small\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0PaI2Odffj0F",
    "outputId": "dfb5b2e1-f138-4224-d8fc-09ba1746f2dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): MarianForCausalLM(\n",
       "    (model): MarianDecoderWrapper(\n",
       "      (decoder): MarianDecoder(\n",
       "        (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "        (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLU()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=67028, bias=False)\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilroberta-base\", \"Helsinki-NLP/opus-mt-en-nl\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8o5qdiagbh2",
    "outputId": "fdab7ef9-8ea0-4005-dcf5-93abcaad2254"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderConfig {\n",
       "  \"decoder\": {\n",
       "    \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-nl\",\n",
       "    \"_num_labels\": 3,\n",
       "    \"activation_dropout\": 0.0,\n",
       "    \"activation_function\": \"swish\",\n",
       "    \"add_bias_logits\": false,\n",
       "    \"add_cross_attention\": true,\n",
       "    \"add_final_layer_norm\": false,\n",
       "    \"architectures\": [\n",
       "      \"MarianMTModel\"\n",
       "    ],\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": [\n",
       "      [\n",
       "        67027\n",
       "      ]\n",
       "    ],\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classif_dropout\": 0.0,\n",
       "    \"classifier_dropout\": 0.0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"d_model\": 512,\n",
       "    \"decoder_attention_heads\": 8,\n",
       "    \"decoder_ffn_dim\": 2048,\n",
       "    \"decoder_layerdrop\": 0.0,\n",
       "    \"decoder_layers\": 6,\n",
       "    \"decoder_start_token_id\": 67027,\n",
       "    \"decoder_vocab_size\": 67028,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"dropout\": 0.1,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_attention_heads\": 8,\n",
       "    \"encoder_ffn_dim\": 2048,\n",
       "    \"encoder_layerdrop\": 0.0,\n",
       "    \"encoder_layers\": 6,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 0,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": 0,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\",\n",
       "      \"2\": \"LABEL_2\"\n",
       "    },\n",
       "    \"init_std\": 0.02,\n",
       "    \"is_decoder\": true,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1,\n",
       "      \"LABEL_2\": 2\n",
       "    },\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 512,\n",
       "    \"max_position_embeddings\": 512,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"marian\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"normalize_before\": false,\n",
       "    \"normalize_embedding\": false,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 4,\n",
       "    \"num_hidden_layers\": 6,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 67027,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"scale_embedding\": true,\n",
       "    \"sep_token_id\": null,\n",
       "    \"share_encoder_decoder_embeddings\": true,\n",
       "    \"static_position_embeddings\": true,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 67028\n",
       "  },\n",
       "  \"encoder\": {\n",
       "    \"_name_or_path\": \"distilroberta-base\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"RobertaForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 2,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 514,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"roberta\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 6,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 1,\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"type_vocab_size\": 1,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 50265\n",
       "  },\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"model_type\": \"encoder-decoder\",\n",
       "  \"transformers_version\": \"4.43.0.dev0\"\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ml1eZC5FIPWw"
   },
   "outputs": [],
   "source": [
    "model.base_model.config.encoder.num_hidden_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "QHm4VdrLZVev"
   },
   "outputs": [],
   "source": [
    "model.tie_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "EilK2ZWBVn2-"
   },
   "outputs": [],
   "source": [
    "model.base_model.config.decoder_start_token_id = model.base_model.config.decoder.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DzGm_QPXHvI4"
   },
   "outputs": [],
   "source": [
    "model.base_model.config.bos_token_id = tokenizer.bos_token_id\n",
    "model.base_model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.base_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "V0rvsHv7HF6T"
   },
   "outputs": [],
   "source": [
    "model.config.max_length =512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1GR3W0pOs7Na",
    "outputId": "1d9788b6-7a20-4a91-e1a6-8b920e6c27e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 512}\n",
      "Your generation config was originally created from the model config, but the model config has changed since then. Unless you pass the `generation_config` argument to this model's `generate` calls, they will revert to the legacy behavior where the base `generate` parameterization is loaded from the model config instead. To avoid this behavior and this warning, we recommend you to overwrite the generation config model attribute before calling the model's `save_pretrained`, preferably also removing any generation kwargs from the model config. This warning will be raised to an exception in v4.41.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(\"distilroberta_marian_base_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpsg2UqTda2a"
   },
   "outputs": [],
   "source": [
    "model=AutoModelForSeq2SeqLM.from_pretrained(\"./distilroberta_marian_training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvxtV52E3V3D",
    "outputId": "15bcadec-ca9f-436f-faf2-6f7afb771d89"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): MarianForCausalLM(\n",
       "    (model): MarianDecoderWrapper(\n",
       "      (decoder): MarianDecoder(\n",
       "        (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "        (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLU()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=67028, bias=False)\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./distilroberta_marian_base_model\")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kOgQ6XU5oGTP",
    "outputId": "75bcfe16-9b3b-48c2-b23b-690ddb2b63a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.config.decoder_start_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWYFhLfWIU2X",
    "outputId": "12a2270c-baac-4ffa-ce20-73c650724665"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OIG2yWiKIoNz",
    "outputId": "d68aeaa8-1ffb-49a7-a0e9-1cf9ff3b7ec6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZchqrwtkDSw",
    "outputId": "bb572ee2-b9d5-47fe-e508-62fe11aa1809"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderConfig {\n",
       "  \"_name_or_path\": \"./distilroberta_marian_base_model\",\n",
       "  \"architectures\": [\n",
       "    \"EncoderDecoderModel\"\n",
       "  ],\n",
       "  \"bos_token_id\": 0,\n",
       "  \"decoder\": {\n",
       "    \"_name_or_path\": \"Helsinki-NLP/opus-mt-en-nl\",\n",
       "    \"_num_labels\": 3,\n",
       "    \"activation_dropout\": 0.0,\n",
       "    \"activation_function\": \"swish\",\n",
       "    \"add_bias_logits\": false,\n",
       "    \"add_cross_attention\": true,\n",
       "    \"add_final_layer_norm\": false,\n",
       "    \"architectures\": [\n",
       "      \"MarianMTModel\"\n",
       "    ],\n",
       "    \"attention_dropout\": 0.0,\n",
       "    \"bad_words_ids\": [\n",
       "      [\n",
       "        67027\n",
       "      ]\n",
       "    ],\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classif_dropout\": 0.0,\n",
       "    \"classifier_dropout\": 0.0,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"d_model\": 512,\n",
       "    \"decoder_attention_heads\": 8,\n",
       "    \"decoder_ffn_dim\": 2048,\n",
       "    \"decoder_layerdrop\": 0.0,\n",
       "    \"decoder_layers\": 6,\n",
       "    \"decoder_start_token_id\": 67027,\n",
       "    \"decoder_vocab_size\": 67028,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"dropout\": 0.1,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_attention_heads\": 8,\n",
       "    \"encoder_ffn_dim\": 2048,\n",
       "    \"encoder_layerdrop\": 0.0,\n",
       "    \"encoder_layers\": 6,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 0,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": 0,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\",\n",
       "      \"2\": \"LABEL_2\"\n",
       "    },\n",
       "    \"init_std\": 0.02,\n",
       "    \"is_decoder\": true,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1,\n",
       "      \"LABEL_2\": 2\n",
       "    },\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 512,\n",
       "    \"max_position_embeddings\": 512,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"marian\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"normalize_before\": false,\n",
       "    \"normalize_embedding\": false,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 4,\n",
       "    \"num_hidden_layers\": 6,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 67027,\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"scale_embedding\": true,\n",
       "    \"sep_token_id\": null,\n",
       "    \"share_encoder_decoder_embeddings\": true,\n",
       "    \"static_position_embeddings\": true,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 67028\n",
       "  },\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"encoder\": {\n",
       "    \"_name_or_path\": \"distilroberta-base\",\n",
       "    \"add_cross_attention\": false,\n",
       "    \"architectures\": [\n",
       "      \"RobertaForMaskedLM\"\n",
       "    ],\n",
       "    \"attention_probs_dropout_prob\": 0.1,\n",
       "    \"bad_words_ids\": null,\n",
       "    \"begin_suppress_tokens\": null,\n",
       "    \"bos_token_id\": 0,\n",
       "    \"chunk_size_feed_forward\": 0,\n",
       "    \"classifier_dropout\": null,\n",
       "    \"cross_attention_hidden_size\": null,\n",
       "    \"decoder_start_token_id\": null,\n",
       "    \"diversity_penalty\": 0.0,\n",
       "    \"do_sample\": false,\n",
       "    \"early_stopping\": false,\n",
       "    \"encoder_no_repeat_ngram_size\": 0,\n",
       "    \"eos_token_id\": 2,\n",
       "    \"exponential_decay_length_penalty\": null,\n",
       "    \"finetuning_task\": null,\n",
       "    \"forced_bos_token_id\": null,\n",
       "    \"forced_eos_token_id\": null,\n",
       "    \"hidden_act\": \"gelu\",\n",
       "    \"hidden_dropout_prob\": 0.1,\n",
       "    \"hidden_size\": 768,\n",
       "    \"id2label\": {\n",
       "      \"0\": \"LABEL_0\",\n",
       "      \"1\": \"LABEL_1\"\n",
       "    },\n",
       "    \"initializer_range\": 0.02,\n",
       "    \"intermediate_size\": 3072,\n",
       "    \"is_decoder\": false,\n",
       "    \"is_encoder_decoder\": false,\n",
       "    \"label2id\": {\n",
       "      \"LABEL_0\": 0,\n",
       "      \"LABEL_1\": 1\n",
       "    },\n",
       "    \"layer_norm_eps\": 1e-05,\n",
       "    \"length_penalty\": 1.0,\n",
       "    \"max_length\": 20,\n",
       "    \"max_position_embeddings\": 514,\n",
       "    \"min_length\": 0,\n",
       "    \"model_type\": \"roberta\",\n",
       "    \"no_repeat_ngram_size\": 0,\n",
       "    \"num_attention_heads\": 12,\n",
       "    \"num_beam_groups\": 1,\n",
       "    \"num_beams\": 1,\n",
       "    \"num_hidden_layers\": 6,\n",
       "    \"num_return_sequences\": 1,\n",
       "    \"output_attentions\": false,\n",
       "    \"output_hidden_states\": false,\n",
       "    \"output_scores\": false,\n",
       "    \"pad_token_id\": 1,\n",
       "    \"position_embedding_type\": \"absolute\",\n",
       "    \"prefix\": null,\n",
       "    \"problem_type\": null,\n",
       "    \"pruned_heads\": {},\n",
       "    \"remove_invalid_values\": false,\n",
       "    \"repetition_penalty\": 1.0,\n",
       "    \"return_dict\": true,\n",
       "    \"return_dict_in_generate\": false,\n",
       "    \"sep_token_id\": null,\n",
       "    \"suppress_tokens\": null,\n",
       "    \"task_specific_params\": null,\n",
       "    \"temperature\": 1.0,\n",
       "    \"tf_legacy_loss\": false,\n",
       "    \"tie_encoder_decoder\": false,\n",
       "    \"tie_word_embeddings\": true,\n",
       "    \"tokenizer_class\": null,\n",
       "    \"top_k\": 50,\n",
       "    \"top_p\": 1.0,\n",
       "    \"torch_dtype\": null,\n",
       "    \"torchscript\": false,\n",
       "    \"type_vocab_size\": 1,\n",
       "    \"typical_p\": 1.0,\n",
       "    \"use_bfloat16\": false,\n",
       "    \"use_cache\": true,\n",
       "    \"vocab_size\": 50265\n",
       "  },\n",
       "  \"eos_token_id\": 2,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"max_length\": 512,\n",
       "  \"model_type\": \"encoder-decoder\",\n",
       "  \"pad_token_id\": 1,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.43.0.dev0\"\n",
       "}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJNVStnrjr8_"
   },
   "source": [
    "# **Evaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ta7Kc7IbNsgq",
    "outputId": "9fa4413b-4f43-48c4-8843-3808de724b7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/distilroberta_marian_base_model/Utils\n"
     ]
    }
   ],
   "source": [
    "%cd /content/distilroberta_marian_base_model/Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_oqUC-1Pwd1",
    "outputId": "e58e1d9b-afcd-4dd5-a0e9-2359d41784c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'evaluator'...\n",
      "fatal: could not read Username for 'https://github.com': No such device or address\n",
      "[Errno 2] No such file or directory: 'evaluator'\n",
      "/content/distilroberta_marian_base_model/Utils/evaluate\n",
      "Obtaining file:///content/distilroberta_marian_base_model/Utils/evaluate\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (2.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (1.25.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (2.0.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (2023.6.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (0.23.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.2) (24.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (16.1.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (0.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (3.9.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate==0.4.2) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate==0.4.2) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate==0.4.2) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.2) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate==0.4.2) (2024.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.2) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.2) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.2) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.2) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.2) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate==0.4.2) (4.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate==0.4.2) (1.16.0)\n",
      "Installing collected packages: evaluate\n",
      "  Attempting uninstall: evaluate\n",
      "    Found existing installation: evaluate 0.4.2\n",
      "    Uninstalling evaluate-0.4.2:\n",
      "      Successfully uninstalled evaluate-0.4.2\n",
      "  Running setup.py develop for evaluate\n",
      "Successfully installed evaluate-0.4.2\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/evaluate.git\n",
    "%cd evaluate\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "MyBQIW1DSBky"
   },
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "id": "brMxQ32GV5gA",
    "outputId": "435be0f8-395b-4ded-be35-a809ae4c7d89"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'metrics' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-76cc0420941f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'metrics' (unknown location)",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from metrics import load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "mHWhUqgyWqiN"
   },
   "outputs": [],
   "source": [
    "from evaluate import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "EFOu47xYXzzS"
   },
   "outputs": [],
   "source": [
    "from metrics import sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "NbOSj8awXPTc",
    "outputId": "df6b7219-dd02-475a-b7c8-f0674ab4ce7b"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8d8b00fed6cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mMetric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sacrebleu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'load_metric' is not defined"
     ]
    }
   ],
   "source": [
    "Metric = load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "ygEx8goOgfIy",
    "outputId": "41ed6f58-89c7-42ee-fb8a-e153cabf85a9"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'evaluate.metrics' has no attribute 'CodeGenerationEvaluator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e154b065a488>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCodeGenerationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msmooth_bleu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'evaluate.metrics' has no attribute 'CodeGenerationEvaluator'"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "evaluator = metrics.CodeGenerationEvaluator(tokenizer, device,smooth_bleu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WtUzOY89cpQw",
    "outputId": "2588010a-b49f-4c24-f14e-d98286e137ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYvdujyfro1i"
   },
   "source": [
    "\n",
    "\n",
    "# **Prepare the data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "4ORxNxB7bINN"
   },
   "outputs": [],
   "source": [
    "encoder_length = 32\n",
    "decoder_length = 32\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "# map data correctly\n",
    "def map_to_encoder_decoder_inputs(batch):\n",
    "    inputs = tokenizer(batch[\"Actions\"], padding=\"max_length\", truncation=True, max_length=encoder_length)\n",
    "    outputs = tokenizer(batch[\"Code Groovy\"], padding=\"max_length\", truncation=True, max_length=decoder_length)\n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    batch[\"decoder_input_ids\"] = outputs.input_ids\n",
    "    batch[\"labels\"] = outputs.input_ids.copy()\n",
    "    batch[\"decoder_attention_mask\"] = outputs.attention_mask\n",
    "\n",
    "    \"\"\"\n",
    "    # complicated list comprehension here because pad_token_id alone is not good enough to know whether label should be excluded or not\n",
    "    batch[\"labels\"] = [\n",
    "        [-100 if mask == 0 else token for mask, token in mask_and_tokens] for mask_and_tokens in [zip(masks, labels) for masks, labels in zip(batch[\"decoder_attention_mask\"], batch[\"labels\"])]\n",
    "    ]\n",
    "    \"\"\"\n",
    "    assert all([len(x) == encoder_length for x in inputs.input_ids])\n",
    "    assert all([len(x) == decoder_length for x in outputs.input_ids])\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "7d05d1de0e3045cc80fb7c227ca26c59",
      "a8d4a065b23f49b49658ab5549975cab",
      "ce7d97731a1c417c80a23a5a3baadf25",
      "f4621dc76cec4719b17cd28affb6bea2",
      "e8d5fc93a0414eb2a72f5acf63907d0f",
      "f8329901fac044609984bea6ce1b433e",
      "b431df0658bf4bb394db58f273ed1d70",
      "aec2f7dbbe654edf898c2fae73831d0d",
      "5c29a4db804e4785a30ab9ad3d15b598",
      "366b6e8cbbd74087acd6e4a08e844249",
      "f3d1382e735d4660baa87509a447b2d6"
     ]
    },
    "id": "5p_GH0x5a-hY",
    "outputId": "d243a708-8154-4fa1-c00f-53e8a0bdad2c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d05d1de0e3045cc80fb7c227ca26c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/166 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make train dataset ready\n",
    "train_data = train_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=1, remove_columns=['Actions', 'Code Groovy'],\n",
    ")\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "zUWfkeRqoCdW",
    "outputId": "6178f9fd-5122-44dd-f35a-c4f37c95ff7f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6062287b1ec9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# same for validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m val_data = val_dataset.map(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mmap_to_encoder_decoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Actions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Code Groovy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      5\u001b[0m val_data.set_format(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# same for validation dataset\n",
    "val_data = val_dataset.map(\n",
    "    map_to_encoder_decoder_inputs, batched=True, batch_size=1, remove_columns=['Actions', 'Code Groovy'],\n",
    ")\n",
    "val_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "tHv_U6xgz8Y0"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer,\n",
    "                                       max_length=512,padding=True, ####new\n",
    "                                       model = model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhetWPjLkQ5P"
   },
   "source": [
    "# **Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 670
    },
    "id": "qF6rhycZPr2m",
    "outputId": "9ee3ac2b-69cc-4682-bbc9-8eda7f1b20e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1504: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-9a900b94a288>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# set training arguments - these params are not really tuned, feel free to change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m training_args = Seq2SeqTrainingArguments(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./distilroberta-marian-training\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msave_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args_seq2seq.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, eval_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, restore_callback_states_from_checkpoint, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by...\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1701\u001b[0m         \u001b[0;31m# Initialize device before we proceed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1703\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorchdynamo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36mdevice\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2185\u001b[0m         \"\"\"\n\u001b[1;32m   2186\u001b[0m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"torch\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_devices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2189\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcached\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mcached\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\u001b[0m in \u001b[0;36m_setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sagemaker_mp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2066\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2067\u001b[0;31m                 raise ImportError(\n\u001b[0m\u001b[1;32m   2068\u001b[0m                     \u001b[0;34mf\"Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m                     \u001b[0;34m\"Please run `pip install transformers[torch]` or `pip install accelerate -U`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: Using the `Trainer` with `PyTorch` requires `accelerate>=0.21.0`: Please run `pip install transformers[torch]` or `pip install accelerate -U`",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# set training arguments - these params are not really tuned, feel free to change\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./distilroberta-marian-training\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    predict_with_generate=True,\n",
    "    num_train_epochs=10,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    fp16=True,\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate = 1e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio = 0.05,\n",
    "    seed = 1995,\n",
    "    save_total_limit = 2,\n",
    "    load_best_model_at_end = True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "#\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=evaluator,\n",
    "    data_collator = data_collator,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset =val_data,\n",
    "\n",
    ")\n",
    "#    save_total_limit=3,\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FKFZg62ubLui",
    "outputId": "d0a6620f-0df8-4aba-b494-d512093f0e3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'accelerate'...\n",
      "remote: Enumerating objects: 13253, done.\u001b[K\n",
      "remote: Counting objects: 100% (4833/4833), done.\u001b[K\n",
      "remote: Compressing objects: 100% (553/553), done.\u001b[K\n",
      "remote: Total 13253 (delta 4511), reused 4360 (delta 4246), pack-reused 8420\u001b[K\n",
      "Receiving objects: 100% (13253/13253), 4.37 MiB | 4.14 MiB/s, done.\n",
      "Resolving deltas: 100% (9191/9191), done.\n",
      "/content/distilroberta_marian_base_model/Utils/accelerate\n",
      "Obtaining file:///content/distilroberta_marian_base_model/Utils/accelerate\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (2.3.0+cu121)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (0.23.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.32.0.dev0) (0.4.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (1.12.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.32.0.dev0) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.32.0.dev0) (12.5.82)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.32.0.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.32.0.dev0) (4.66.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.32.0.dev0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.32.0.dev0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.32.0.dev0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.32.0.dev0) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.32.0.dev0) (2024.6.2)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.32.0.dev0) (1.3.0)\n",
      "Building wheels for collected packages: accelerate\n",
      "  Building editable for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for accelerate: filename=accelerate-0.32.0.dev0-0.editable-py3-none-any.whl size=12178 sha256=11d33eafd43071e2941dbafbfbc600462c8ba55c584f9a6914e5ad95ab7ce190\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-rfyl_rd3/wheels/94/11/e9/0baa1bf56f32bc5dd8e58f79a35a4c315ec8e69d592fdd1f53\n",
      "Successfully built accelerate\n",
      "Installing collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.31.0\n",
      "    Uninstalling accelerate-0.31.0:\n",
      "      Successfully uninstalled accelerate-0.31.0\n",
      "Successfully installed accelerate-0.32.0.dev0\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/huggingface/accelerate\n",
    "%cd accelerate\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WA3z0NRHfUEO",
    "outputId": "925f1b22-033b-4ad1-dc39-58f407028aa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160000\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160000' max='160000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160000/160000 3:49:39, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sacrebleu</th>\n",
       "      <th>Bleu-unigram-precision</th>\n",
       "      <th>Bleu-bigram-precision</th>\n",
       "      <th>Bleu-trigram-precision</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Sacre-unigram-precision</th>\n",
       "      <th>Sacre-bigram-precision</th>\n",
       "      <th>Sacre-trigram-precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.259300</td>\n",
       "      <td>1.112706</td>\n",
       "      <td>12.093183</td>\n",
       "      <td>11.579566</td>\n",
       "      <td>31.111363</td>\n",
       "      <td>16.870151</td>\n",
       "      <td>8.920128</td>\n",
       "      <td>15.495665</td>\n",
       "      <td>46.322058</td>\n",
       "      <td>31.125087</td>\n",
       "      <td>15.391544</td>\n",
       "      <td>8.305034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.703100</td>\n",
       "      <td>0.677957</td>\n",
       "      <td>41.742606</td>\n",
       "      <td>44.027506</td>\n",
       "      <td>65.590112</td>\n",
       "      <td>48.593888</td>\n",
       "      <td>35.766605</td>\n",
       "      <td>41.896389</td>\n",
       "      <td>68.156894</td>\n",
       "      <td>66.294416</td>\n",
       "      <td>49.262837</td>\n",
       "      <td>38.177507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.509812</td>\n",
       "      <td>54.167319</td>\n",
       "      <td>56.049901</td>\n",
       "      <td>72.915196</td>\n",
       "      <td>59.627677</td>\n",
       "      <td>49.061595</td>\n",
       "      <td>57.159857</td>\n",
       "      <td>78.445924</td>\n",
       "      <td>73.176988</td>\n",
       "      <td>60.185559</td>\n",
       "      <td>51.079781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.404900</td>\n",
       "      <td>0.427548</td>\n",
       "      <td>64.497157</td>\n",
       "      <td>66.055326</td>\n",
       "      <td>80.985618</td>\n",
       "      <td>69.347209</td>\n",
       "      <td>59.596095</td>\n",
       "      <td>64.131359</td>\n",
       "      <td>82.926478</td>\n",
       "      <td>80.996735</td>\n",
       "      <td>69.736563</td>\n",
       "      <td>61.410788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.354300</td>\n",
       "      <td>0.375074</td>\n",
       "      <td>67.809657</td>\n",
       "      <td>69.669556</td>\n",
       "      <td>85.101928</td>\n",
       "      <td>74.699690</td>\n",
       "      <td>66.121759</td>\n",
       "      <td>67.807849</td>\n",
       "      <td>85.393115</td>\n",
       "      <td>85.603544</td>\n",
       "      <td>75.808799</td>\n",
       "      <td>68.707569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.288900</td>\n",
       "      <td>0.353043</td>\n",
       "      <td>70.715418</td>\n",
       "      <td>72.363218</td>\n",
       "      <td>85.982832</td>\n",
       "      <td>76.520785</td>\n",
       "      <td>68.230897</td>\n",
       "      <td>70.014224</td>\n",
       "      <td>86.724613</td>\n",
       "      <td>86.516071</td>\n",
       "      <td>77.765499</td>\n",
       "      <td>70.842627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.246200</td>\n",
       "      <td>0.336535</td>\n",
       "      <td>72.808929</td>\n",
       "      <td>74.193876</td>\n",
       "      <td>87.822066</td>\n",
       "      <td>79.225439</td>\n",
       "      <td>71.688019</td>\n",
       "      <td>71.528419</td>\n",
       "      <td>87.857990</td>\n",
       "      <td>88.340898</td>\n",
       "      <td>80.291643</td>\n",
       "      <td>74.117944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.237300</td>\n",
       "      <td>0.322036</td>\n",
       "      <td>73.595702</td>\n",
       "      <td>75.128414</td>\n",
       "      <td>87.367050</td>\n",
       "      <td>78.658388</td>\n",
       "      <td>71.495715</td>\n",
       "      <td>72.065756</td>\n",
       "      <td>88.114859</td>\n",
       "      <td>87.716691</td>\n",
       "      <td>79.659153</td>\n",
       "      <td>73.703019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.211600</td>\n",
       "      <td>0.313317</td>\n",
       "      <td>74.852383</td>\n",
       "      <td>76.121581</td>\n",
       "      <td>87.902090</td>\n",
       "      <td>79.727924</td>\n",
       "      <td>72.759857</td>\n",
       "      <td>72.908647</td>\n",
       "      <td>88.657585</td>\n",
       "      <td>88.274358</td>\n",
       "      <td>80.712950</td>\n",
       "      <td>74.919852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.205200</td>\n",
       "      <td>0.313937</td>\n",
       "      <td>75.167198</td>\n",
       "      <td>76.455672</td>\n",
       "      <td>88.299956</td>\n",
       "      <td>80.385931</td>\n",
       "      <td>73.533084</td>\n",
       "      <td>73.393079</td>\n",
       "      <td>88.900018</td>\n",
       "      <td>88.875598</td>\n",
       "      <td>81.454827</td>\n",
       "      <td>75.773004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-16000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-16000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-16000/special_tokens_map.json\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-32000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-32000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-32000/special_tokens_map.json\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-48000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-48000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-16000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-64000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-64000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-32000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-80000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-80000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-48000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-96000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-96000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-64000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-112000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-112000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-80000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-128000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-128000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-96000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-144000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-144000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-112000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training/checkpoint-160000\n",
      "Configuration saved in ./distilroberta-marian-training/checkpoint-160000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training/checkpoint-160000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training/checkpoint-128000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./distilroberta-marian-training/checkpoint-144000 (score: 0.31331682205200195).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160000, training_loss=0.617371564245224, metrics={'train_runtime': 13782.5148, 'train_samples_per_second': 11.609, 'train_steps_per_second': 11.609, 'total_flos': 2111609241600000.0, 'train_loss': 0.617371564245224, 'epoch': 10.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distilRoBERTa-Marian-Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a6NaXLCG2zC_",
    "outputId": "2b133de1-617c-428d-c643-9b52940e4a5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160000\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='112366' max='160000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [112366/160000 2:38:03 < 1:07:00, 11.85 it/s, Epoch 7.02/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sacrebleu</th>\n",
       "      <th>Bleu-unigram-precision</th>\n",
       "      <th>Bleu-bigram-precision</th>\n",
       "      <th>Bleu-trigram-precision</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Sacre-unigram-precision</th>\n",
       "      <th>Sacre-bigram-precision</th>\n",
       "      <th>Sacre-trigram-precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.327421</td>\n",
       "      <td>73.834146</td>\n",
       "      <td>75.400138</td>\n",
       "      <td>87.714721</td>\n",
       "      <td>79.129056</td>\n",
       "      <td>71.865909</td>\n",
       "      <td>72.504967</td>\n",
       "      <td>88.370630</td>\n",
       "      <td>87.781732</td>\n",
       "      <td>79.870557</td>\n",
       "      <td>73.810987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.310745</td>\n",
       "      <td>75.361611</td>\n",
       "      <td>76.756218</td>\n",
       "      <td>88.220579</td>\n",
       "      <td>80.485721</td>\n",
       "      <td>73.762994</td>\n",
       "      <td>73.958711</td>\n",
       "      <td>88.990059</td>\n",
       "      <td>88.419592</td>\n",
       "      <td>81.322914</td>\n",
       "      <td>75.749352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.302277</td>\n",
       "      <td>76.961354</td>\n",
       "      <td>78.115608</td>\n",
       "      <td>88.083564</td>\n",
       "      <td>80.326891</td>\n",
       "      <td>73.898946</td>\n",
       "      <td>74.702454</td>\n",
       "      <td>89.584204</td>\n",
       "      <td>88.194647</td>\n",
       "      <td>81.121294</td>\n",
       "      <td>75.684147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.290385</td>\n",
       "      <td>76.975630</td>\n",
       "      <td>78.478887</td>\n",
       "      <td>87.635092</td>\n",
       "      <td>80.007111</td>\n",
       "      <td>73.545186</td>\n",
       "      <td>75.174441</td>\n",
       "      <td>89.777397</td>\n",
       "      <td>87.882887</td>\n",
       "      <td>80.885656</td>\n",
       "      <td>75.516788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.285707</td>\n",
       "      <td>77.774790</td>\n",
       "      <td>79.146762</td>\n",
       "      <td>89.587191</td>\n",
       "      <td>82.581591</td>\n",
       "      <td>76.656020</td>\n",
       "      <td>76.107073</td>\n",
       "      <td>90.268323</td>\n",
       "      <td>89.641513</td>\n",
       "      <td>83.304023</td>\n",
       "      <td>78.393693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.285543</td>\n",
       "      <td>79.110200</td>\n",
       "      <td>80.092983</td>\n",
       "      <td>91.315383</td>\n",
       "      <td>85.003721</td>\n",
       "      <td>79.313753</td>\n",
       "      <td>76.604302</td>\n",
       "      <td>90.700086</td>\n",
       "      <td>91.418211</td>\n",
       "      <td>85.595878</td>\n",
       "      <td>81.089582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.278289</td>\n",
       "      <td>79.182430</td>\n",
       "      <td>80.511640</td>\n",
       "      <td>90.082555</td>\n",
       "      <td>83.658299</td>\n",
       "      <td>78.106836</td>\n",
       "      <td>76.781054</td>\n",
       "      <td>90.757730</td>\n",
       "      <td>90.432282</td>\n",
       "      <td>84.624697</td>\n",
       "      <td>80.150506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-16000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-16000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-16000/special_tokens_map.json\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-32000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-32000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-32000/special_tokens_map.json\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-48000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-48000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-16000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-64000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-64000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-32000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-80000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-80000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-48000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-96000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-96000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-64000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-112000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-112000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-80000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160000' max='160000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160000/160000 3:45:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sacrebleu</th>\n",
       "      <th>Bleu-unigram-precision</th>\n",
       "      <th>Bleu-bigram-precision</th>\n",
       "      <th>Bleu-trigram-precision</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Sacre-unigram-precision</th>\n",
       "      <th>Sacre-bigram-precision</th>\n",
       "      <th>Sacre-trigram-precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.327421</td>\n",
       "      <td>73.834146</td>\n",
       "      <td>75.400138</td>\n",
       "      <td>87.714721</td>\n",
       "      <td>79.129056</td>\n",
       "      <td>71.865909</td>\n",
       "      <td>72.504967</td>\n",
       "      <td>88.370630</td>\n",
       "      <td>87.781732</td>\n",
       "      <td>79.870557</td>\n",
       "      <td>73.810987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.310745</td>\n",
       "      <td>75.361611</td>\n",
       "      <td>76.756218</td>\n",
       "      <td>88.220579</td>\n",
       "      <td>80.485721</td>\n",
       "      <td>73.762994</td>\n",
       "      <td>73.958711</td>\n",
       "      <td>88.990059</td>\n",
       "      <td>88.419592</td>\n",
       "      <td>81.322914</td>\n",
       "      <td>75.749352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.302277</td>\n",
       "      <td>76.961354</td>\n",
       "      <td>78.115608</td>\n",
       "      <td>88.083564</td>\n",
       "      <td>80.326891</td>\n",
       "      <td>73.898946</td>\n",
       "      <td>74.702454</td>\n",
       "      <td>89.584204</td>\n",
       "      <td>88.194647</td>\n",
       "      <td>81.121294</td>\n",
       "      <td>75.684147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.290385</td>\n",
       "      <td>76.975630</td>\n",
       "      <td>78.478887</td>\n",
       "      <td>87.635092</td>\n",
       "      <td>80.007111</td>\n",
       "      <td>73.545186</td>\n",
       "      <td>75.174441</td>\n",
       "      <td>89.777397</td>\n",
       "      <td>87.882887</td>\n",
       "      <td>80.885656</td>\n",
       "      <td>75.516788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.285707</td>\n",
       "      <td>77.774790</td>\n",
       "      <td>79.146762</td>\n",
       "      <td>89.587191</td>\n",
       "      <td>82.581591</td>\n",
       "      <td>76.656020</td>\n",
       "      <td>76.107073</td>\n",
       "      <td>90.268323</td>\n",
       "      <td>89.641513</td>\n",
       "      <td>83.304023</td>\n",
       "      <td>78.393693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.285543</td>\n",
       "      <td>79.110200</td>\n",
       "      <td>80.092983</td>\n",
       "      <td>91.315383</td>\n",
       "      <td>85.003721</td>\n",
       "      <td>79.313753</td>\n",
       "      <td>76.604302</td>\n",
       "      <td>90.700086</td>\n",
       "      <td>91.418211</td>\n",
       "      <td>85.595878</td>\n",
       "      <td>81.089582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.278289</td>\n",
       "      <td>79.182430</td>\n",
       "      <td>80.511640</td>\n",
       "      <td>90.082555</td>\n",
       "      <td>83.658299</td>\n",
       "      <td>78.106836</td>\n",
       "      <td>76.781054</td>\n",
       "      <td>90.757730</td>\n",
       "      <td>90.432282</td>\n",
       "      <td>84.624697</td>\n",
       "      <td>80.150506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.273894</td>\n",
       "      <td>79.824580</td>\n",
       "      <td>80.932717</td>\n",
       "      <td>90.060567</td>\n",
       "      <td>83.858841</td>\n",
       "      <td>78.543903</td>\n",
       "      <td>77.037467</td>\n",
       "      <td>90.799239</td>\n",
       "      <td>89.974420</td>\n",
       "      <td>84.340899</td>\n",
       "      <td>80.058651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.272633</td>\n",
       "      <td>79.768941</td>\n",
       "      <td>80.984759</td>\n",
       "      <td>89.271580</td>\n",
       "      <td>82.896155</td>\n",
       "      <td>77.527480</td>\n",
       "      <td>76.989931</td>\n",
       "      <td>90.819831</td>\n",
       "      <td>89.348652</td>\n",
       "      <td>83.550858</td>\n",
       "      <td>79.182246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.274617</td>\n",
       "      <td>80.256615</td>\n",
       "      <td>81.401178</td>\n",
       "      <td>90.100291</td>\n",
       "      <td>83.935694</td>\n",
       "      <td>78.801591</td>\n",
       "      <td>77.334802</td>\n",
       "      <td>91.046787</td>\n",
       "      <td>90.233107</td>\n",
       "      <td>84.695102</td>\n",
       "      <td>80.591181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-128000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-128000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-96000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-144000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-144000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-112000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2317: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-160000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-160000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-160000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-128000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./distilroberta-marian-training1/checkpoint-144000 (score: 0.2726329565048218).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160000, training_loss=0.13924457960128783, metrics={'train_runtime': 13506.2835, 'train_samples_per_second': 11.846, 'train_steps_per_second': 11.846, 'total_flos': 2111609241600000.0, 'train_loss': 0.13924457960128783, 'epoch': 10.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distilRoBERTa-Marian-Training-old\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bl91Iv4hBZLP",
    "outputId": "4c8423a7-e872-4b15-d93a-9965ec842542"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 16000\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 160000\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160000' max='160000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160000/160000 3:37:13, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sacrebleu</th>\n",
       "      <th>Bleu-unigram-precision</th>\n",
       "      <th>Bleu-bigram-precision</th>\n",
       "      <th>Bleu-trigram-precision</th>\n",
       "      <th>Rouge-2</th>\n",
       "      <th>Rouge-l</th>\n",
       "      <th>Sacre-unigram-precision</th>\n",
       "      <th>Sacre-bigram-precision</th>\n",
       "      <th>Sacre-trigram-precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.267400</td>\n",
       "      <td>0.327421</td>\n",
       "      <td>73.834146</td>\n",
       "      <td>75.400138</td>\n",
       "      <td>87.714721</td>\n",
       "      <td>79.129056</td>\n",
       "      <td>71.865909</td>\n",
       "      <td>72.643151</td>\n",
       "      <td>88.442203</td>\n",
       "      <td>87.781732</td>\n",
       "      <td>79.870557</td>\n",
       "      <td>73.810987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.193600</td>\n",
       "      <td>0.310745</td>\n",
       "      <td>75.361611</td>\n",
       "      <td>76.756218</td>\n",
       "      <td>88.220579</td>\n",
       "      <td>80.485721</td>\n",
       "      <td>73.762994</td>\n",
       "      <td>74.095062</td>\n",
       "      <td>89.050357</td>\n",
       "      <td>88.419592</td>\n",
       "      <td>81.322914</td>\n",
       "      <td>75.749352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.302277</td>\n",
       "      <td>76.961354</td>\n",
       "      <td>78.115608</td>\n",
       "      <td>88.083564</td>\n",
       "      <td>80.326891</td>\n",
       "      <td>73.898946</td>\n",
       "      <td>74.809115</td>\n",
       "      <td>89.608955</td>\n",
       "      <td>88.194647</td>\n",
       "      <td>81.121294</td>\n",
       "      <td>75.684147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.137100</td>\n",
       "      <td>0.290385</td>\n",
       "      <td>76.975630</td>\n",
       "      <td>78.478887</td>\n",
       "      <td>87.635092</td>\n",
       "      <td>80.007111</td>\n",
       "      <td>73.545186</td>\n",
       "      <td>75.317715</td>\n",
       "      <td>89.815351</td>\n",
       "      <td>87.882887</td>\n",
       "      <td>80.885656</td>\n",
       "      <td>75.516788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.131400</td>\n",
       "      <td>0.285707</td>\n",
       "      <td>77.774790</td>\n",
       "      <td>79.146762</td>\n",
       "      <td>89.587191</td>\n",
       "      <td>82.581591</td>\n",
       "      <td>76.656020</td>\n",
       "      <td>76.216947</td>\n",
       "      <td>90.303216</td>\n",
       "      <td>89.641513</td>\n",
       "      <td>83.304023</td>\n",
       "      <td>78.393693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.285543</td>\n",
       "      <td>79.110200</td>\n",
       "      <td>80.092983</td>\n",
       "      <td>91.315383</td>\n",
       "      <td>85.003721</td>\n",
       "      <td>79.313753</td>\n",
       "      <td>76.722190</td>\n",
       "      <td>90.742864</td>\n",
       "      <td>91.418211</td>\n",
       "      <td>85.595878</td>\n",
       "      <td>81.089582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.278289</td>\n",
       "      <td>79.182430</td>\n",
       "      <td>80.511640</td>\n",
       "      <td>90.082555</td>\n",
       "      <td>83.658299</td>\n",
       "      <td>78.106836</td>\n",
       "      <td>76.926052</td>\n",
       "      <td>90.804006</td>\n",
       "      <td>90.432282</td>\n",
       "      <td>84.624697</td>\n",
       "      <td>80.150506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.094100</td>\n",
       "      <td>0.273894</td>\n",
       "      <td>79.824580</td>\n",
       "      <td>80.932717</td>\n",
       "      <td>90.060567</td>\n",
       "      <td>83.858841</td>\n",
       "      <td>78.543903</td>\n",
       "      <td>77.160399</td>\n",
       "      <td>90.859090</td>\n",
       "      <td>89.974420</td>\n",
       "      <td>84.340899</td>\n",
       "      <td>80.058651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082500</td>\n",
       "      <td>0.272633</td>\n",
       "      <td>79.768941</td>\n",
       "      <td>80.984759</td>\n",
       "      <td>89.271580</td>\n",
       "      <td>82.896155</td>\n",
       "      <td>77.527480</td>\n",
       "      <td>77.136882</td>\n",
       "      <td>90.873266</td>\n",
       "      <td>89.348652</td>\n",
       "      <td>83.550858</td>\n",
       "      <td>79.182246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.077600</td>\n",
       "      <td>0.274617</td>\n",
       "      <td>80.256615</td>\n",
       "      <td>81.401178</td>\n",
       "      <td>90.100291</td>\n",
       "      <td>83.935694</td>\n",
       "      <td>78.801591</td>\n",
       "      <td>77.497709</td>\n",
       "      <td>91.114060</td>\n",
       "      <td>90.233107</td>\n",
       "      <td>84.695102</td>\n",
       "      <td>80.591181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-16000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-16000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-16000/special_tokens_map.json\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-32000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-32000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-32000/special_tokens_map.json\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-48000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-48000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-16000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-64000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-64000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-32000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-80000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-80000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-48000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-96000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-96000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-64000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-112000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-112000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-80000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-128000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-128000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-96000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-144000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-144000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-112000] due to args.save_total_limit\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2319: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  \"`max_length` is ignored when `padding`=`True` and there is no truncation strategy. \"\n",
      "/usr/local/lib/python3.7/dist-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:533: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "The following columns in the evaluation set don't have a corresponding argument in `EncoderDecoderModel.forward` and have been ignored: Unnamed: 0. If Unnamed: 0 are not expected by `EncoderDecoderModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 1\n",
      "Saving model checkpoint to ./distilroberta-marian-training1/checkpoint-160000\n",
      "Configuration saved in ./distilroberta-marian-training1/checkpoint-160000/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/checkpoint-160000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilroberta-marian-training1/checkpoint-128000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./distilroberta-marian-training1/checkpoint-144000 (score: 0.2726329565048218).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160000, training_loss=0.13924457960128783, metrics={'train_runtime': 13035.4538, 'train_samples_per_second': 12.274, 'train_steps_per_second': 12.274, 'total_flos': 2111609241600000.0, 'train_loss': 0.13924457960128783, 'epoch': 10.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distilRoBERTa-Marian-Training-1\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sctuvauTLQg",
    "outputId": "c058597c-9d27-4863-fa0c-bae00b89a3a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./distilroberta-marian-training1\n",
      "Configuration saved in ./distilroberta-marian-training1/config.json\n",
      "Model weights saved in ./distilroberta-marian-training1/pytorch_model.bin\n",
      "tokenizer config file saved in ./distilroberta-marian-training1/tokenizer_config.json\n",
      "Special tokens file saved in ./distilroberta-marian-training1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjzDA4UJVRc3"
   },
   "source": [
    "# **Visualize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IhQfjGo_6Y2X"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYgohOde7ECZ"
   },
   "source": [
    "### **distilRoBERTa-marian**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "id": "gOUtDLCsd7_b",
    "outputId": "be88df14-ea53-4f20-cdef-64f0dbe9b91b"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6007, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir ./distilroberta-Marian-Training/runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5aiwU7SvGOo"
   },
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zUeup-K_kDZT"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "new_model = AutoModelForSeq2SeqLM.from_pretrained(\"./distilroberta-marian-training1/\")\n",
    "new_model.to(\"cuda\")\n",
    "\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(\"./distilroberta-marian-training1/\", use_fast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yth1WAaS37V1",
    "outputId": "5a166c6b-73bb-4a46-bc45-e8b03fce3138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoderModel(\n",
       "  (encoder): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): MarianForCausalLM(\n",
       "    (model): MarianDecoderWrapper(\n",
       "      (decoder): MarianDecoder(\n",
       "        (embed_tokens): Embedding(67028, 512, padding_idx=67027)\n",
       "        (embed_positions): MarianSinusoidalPositionalEmbedding(512, 512)\n",
       "        (layers): ModuleList(\n",
       "          (0): MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): MarianDecoderLayer(\n",
       "            (self_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (activation_fn): SiLUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): MarianAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=512, out_features=67028, bias=False)\n",
       "  )\n",
       "  (enc_to_dec_proj): Linear(in_features=768, out_features=512, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ujXTqkBhRHgh",
    "outputId": "526f3756-e5a5-4636-cedb-872d50a184e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.config.max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6ny8t34jDoE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1E101Hm6jEck"
   },
   "source": [
    "# **Evaluator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8myiUJ-lGmO",
    "outputId": "5f5d5c45-6f90-4a17-c329-692236de2170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian/Utils\n"
     ]
    }
   ],
   "source": [
    "%cd Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200,
     "referenced_widgets": [
      "8725dcdf1b804c6a93d78c6153b166fa",
      "f8ba7da989374ba48c819d0931d7409a",
      "769be8f1dadf44cab5771d334814b68b",
      "489de527d51a45e49943d5fedc9eb7e9",
      "1677fd9f88e948b08e7a7f770c73abab",
      "160d590b0a4d44a99cb75731d29d787b",
      "62b445053fd84f3daf4547b9f6dec98d",
      "3f4c22f7191d48ebadcb98a8a62b4ad9",
      "316ff94579fe48c49248f7767b1a3cb2",
      "d5cde4bea5234d158e8cfe63ef3175ac",
      "47a44799fd9946bc87ffc057ce725437",
      "78da3bdad2f34aa985d4af72db0f7ea7",
      "771cf0b512014fada9647213fb822c1d",
      "84cc57f64e584de094843099c8d41384",
      "a4ddbde4e1e1476d9f80237da102ee6a",
      "a7c900de40a54173aaf6f45949f8b6f3",
      "a1be9004748b4e99b191362efeba9390",
      "5f34c9b3366a4eb6ba1addd1d0137353",
      "053137ce2f694b598b75fc54dc56d27f",
      "d023a09a50094505bf752d7e42ab1532",
      "276e046c19054cc794e4afcbfb3bab87",
      "d4d31c5cc018487d8ec84a8ecde10de8",
      "61a9abd5145b4599a94b7eb0740c7f62",
      "ce63e9942734429f9e1730a6f00a5643",
      "a46278423c0146b7a43baba00bd24a62",
      "1e3287543fec49f281bff052e7d2e07d",
      "7a2edc562b0a47c49608061ada9134b4",
      "e5b04f0c0b1141798f82671c4bcc7f1a",
      "bbf4adc7b5c046d5a46e5c05bb4595a8",
      "8514b667f88c44a5bdf7c7d96af5c8a2",
      "07229f5fc731423391813864a18b5cb3",
      "636bf065ab50407abee10ec045c4f816",
      "d8ea81ed0a614ff3a7030cb1a0ff1035",
      "5a67d10adf25484097cd063c9ada7439",
      "a2a8e36358e74f6abc3f2e80e7c77e17",
      "903c88afd5a648308d7847d8f8a29447",
      "81249daf5ccd41558c41afedd07a3d71",
      "e1304c686ac849c4aaeb4f4f3ee653da",
      "d179d33897a34382904fbb9ade5443bf",
      "a6d3d7ae41c6420992d10ed2689711dc",
      "f655aa47ff0245749d9104d3cd4b1413",
      "8d857025207e4a8e82010fbeb2da51d7",
      "a84d8126ce54400e99042d9453cf3a02",
      "7139605f3b2a409890859b0a9bfe976d"
     ]
    },
    "id": "jxOpBKhWdfla",
    "outputId": "e935fe3a-aa58-45a7-a2e6-832d2b609858"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian/Utils/evaluator.py:32: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.sacre_bleu: Metric = load_metric('sacrebleu')\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8725dcdf1b804c6a93d78c6153b166fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.85k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78da3bdad2f34aa985d4af72db0f7ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.48k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a9abd5145b4599a94b7eb0740c7f62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a67d10adf25484097cd063c9ada7439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "import evaluator\n",
    "evaluator = evaluator.CodeGenerationEvaluator(new_tokenizer, device,smooth_bleu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yr54jYmVjEcn"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "# map data correctly\n",
    "def generate_new_code(batch):\n",
    "    inputs = new_tokenizer(batch[\"nl\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = new_model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = new_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred_code\"] = output_str\n",
    "\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "ecd2b56bd1d649cfb0183b4665eb36d6",
      "287bee6cc9fa40639d9843786ec56c62",
      "c01732c6e7e949bcaa75e5d2b8677a63",
      "0941dec0d338463bbcf6d27d281a394b",
      "4e12d866d32d414ca37a794207e26158",
      "dbba476f424c40038312d723a5e8c77d",
      "4a577ef0459d4d9cafc17b5d82aaa9fb",
      "be52f6d00b364349b798ee6ca28995d0",
      "bbf64fac4d5044bda43abd182da2b8f6",
      "e56973a6fa7140c0a6fc0654b9dc7e6c",
      "ff58a8fb8a354b73b165b739d0f7fac8"
     ]
    },
    "id": "aW-fQq9xd_Ei",
    "outputId": "bbfc3ba0-d66c-44c0-88bc-f87f1e139d87"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecd2b56bd1d649cfb0183b4665eb36d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1805 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py:1261: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 512 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "results = test_dataset.map(generate_new_code, batched=True, batch_size=batch_size)\n",
    "\n",
    "#results = test_dataset.map(generate_new_code, batched=True, batch_size=batch_size, remove_columns=[\"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bg9jBzHg-hdB",
    "outputId": "a96b9274-e474-43ec-9fbb-ad8d7df572cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZPghyh7wo9Q"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "\n",
    "import evaluate\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67,
     "referenced_widgets": [
      "2d6d56af44d5468ca4cd89e523c0d789",
      "b096693119604256bd03a10f2f903fc6",
      "c541e34d137f4ddbbe4941b8f3031698",
      "6d3d8cae0ebb442eb9e123c028b70590",
      "3e3b43d83fe648bd812b0887501cbcb5",
      "814a0d83738248d09783a4a3a095955f",
      "6d165d1957ea42219deaa6c277a4f832",
      "1576756311624e76b0d0e9aea9044f18",
      "5a328a1e71df4a9ab21142885a3ee277",
      "27b8c868bd9344e48469f6d19de59509",
      "9722e321e6c142a6a65212ba3689c821"
     ]
    },
    "id": "bZxA3QjuwT7v",
    "outputId": "7e43a6df-5a3e-41d9-91f2-9aa5b7dd34cd"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d6d56af44d5468ca4cd89e523c0d789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'\\nfor i in range(len(results)):\\n  refs = results[\"code\"][i]\\n  preds = results[\"pred_code\"][i]\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "exact_match = evaluate.load(\"exact_match\")\n",
    "\n",
    "#refs = [\"The cat sat on the mat.\", \"Theaters are great.\", \"It's like comparing oranges and apples.\"]\n",
    "#preds = [\"The cat sat on the mat?\", \"Theaters are great.\", \"It's like comparing apples and oranges.\"]\n",
    "\n",
    "\"\"\"\n",
    "for i in range(len(results)):\n",
    "  refs = results[\"code\"][i]\n",
    "  preds = results[\"pred_code\"][i]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsJnOenL1B-6"
   },
   "outputs": [],
   "source": [
    "refs = results[\"code\"]\n",
    "preds = results[\"pred_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyQJP8ki0hoD",
    "outputId": "d73a7f3c-7f4f-4f35-81b0-afa48d5a30f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.95\n"
     ]
    }
   ],
   "source": [
    "\n",
    "exact_match_results = exact_match.compute(references=refs, predictions=preds, regexes_to_ignore=[\"▁\", \" \"], ignore_case=True, ignore_punctuation=True, ignore_numbers=True)\n",
    "print(round(exact_match_results[\"exact_match\"], 5)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gatvWetJlgJL"
   },
   "outputs": [],
   "source": [
    "with open(\"./references.txt\", 'w') as f:\n",
    "  for i in range(len(results)):\n",
    "    nl = results[\"nl\"][i]\n",
    "    ref = results[\"code\"][i]\n",
    "    pred = results[\"pred_code\"][i]\n",
    "\n",
    "    if pred is not None and pred != \"\":\n",
    "      if ref is not None and ref != \"\":\n",
    "        f.write(ref + '\\n')\n",
    "    else:\n",
    "      continue\n",
    "\n",
    "\n",
    "with open(\"./predictions.txt\", 'w') as f:\n",
    "  for i in range(len(results)):\n",
    "    nl = results[\"nl\"][i]\n",
    "    ref = results[\"code\"][i]\n",
    "    pred = results[\"pred_code\"][i]\n",
    "\n",
    "    if pred is not None and pred != \"\":\n",
    "      if ref is not None and ref != \"\":\n",
    "        f.write(pred + '\\n')\n",
    "    else:\n",
    "      continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRbbazVujEcw"
   },
   "outputs": [],
   "source": [
    "# Ok Evaluator\n",
    "bleu_score =0\n",
    "rouge_score = 0\n",
    "sacre_bleu = 0\n",
    "\n",
    "for i in range(len(results)):\n",
    "  ref = results[\"code\"][i]\n",
    "  pred = results[\"pred_code\"][i]\n",
    "  if pred is not None and pred != \"\":\n",
    "      if ref is not None and ref != \"\":\n",
    "        #bleu_metric = metric.compute(predictions=[pred], references=[[ref]])\n",
    "        #bleu_score  += bleu_metric[\"score\"]\n",
    "\n",
    "        bleu_metric = evaluator.evaluate([pred], [ref])\n",
    "        #bleu_metric = evaluator.evaluateSingle(pred, ref)\n",
    "\n",
    "        #bleu_score += bleu_metric['BLEU-Unigram-Precision']\n",
    "        bleu_score += bleu_metric['BLEU']\n",
    "        rouge_score += bleu_metric['ROUGE-L']\n",
    "        sacre_bleu += bleu_metric['SacreBLEU']\n",
    "        #print(bleu_metric['SacreBLEU'])\n",
    "  else:\n",
    "    continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4y1LccDPRJc-"
   },
   "source": [
    "# **Performance Metrics**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAjUc9xleXTI"
   },
   "source": [
    "**BLEU Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pfKvIRDGjEcx",
    "outputId": "94145512-7408-47fd-b2d7-9422542bc9b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bleu Score: 88.91230715490435 (scale 0-100)\n"
     ]
    }
   ],
   "source": [
    "# Marian  85.13\n",
    "# print(\"Bleu Score: \",bleu_score/len(results))\n",
    "print('Bleu Score: {} (scale 0-100)'.format(bleu_score/len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WApdMa6AeZnS"
   },
   "source": [
    "**SacreBLEU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6ExrnImjEcy",
    "outputId": "e5396ca1-3a66-448c-ab71-7367aa8c5665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sacre Bleu: 74.08303353280789 (scale 0-100)\n"
     ]
    }
   ],
   "source": [
    "# Marian  70.26\n",
    "# print(\"Bleu Score: \",bleu_score/len(results))\n",
    "print('Sacre Bleu: {} (scale 0-100)'.format(sacre_bleu/len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4-EepZCaecsi"
   },
   "source": [
    "**Rouge Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1ErsIXejEcz",
    "outputId": "265d83e5-3556-4e14-bfac-83634aef892b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score: 92.7350696084888 (scale 0-100)\n"
     ]
    }
   ],
   "source": [
    "#Marian  90.07\n",
    "# print(\"Bleu Score: \",bleu_score/len(results))\n",
    "print('ROUGE Score: {} (scale 0-100)'.format(rouge_score/len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xJ4YMtTtOtmW"
   },
   "source": [
    "**CodeBLEU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yhJT_NsiyiZt"
   },
   "outputs": [],
   "source": [
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC6ETJ0u2KRL"
   },
   "outputs": [],
   "source": [
    "import calc_code_bleu\n",
    "ngram_match_score, weighted_ngram_match_score, syntax_match_score, dataflow_match_score, code_bleu_score = calc_code_bleu.calc_code_bleu(\"references.txt\",\"predictions.txt\",\"python\", \"keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Oi6CvK_2GZl"
   },
   "outputs": [],
   "source": [
    "print(\"ngram_match_score: \", ngram_match_score*100)\n",
    "print(\"weighted_ngram_match_score: \", weighted_ngram_match_score*100)\n",
    "print(\"syntax_match_score: \", syntax_match_score*100)\n",
    "print(\"dataflow_match_score: \", dataflow_match_score*100)\n",
    "print(\"code_bleu_score: \", code_bleu_score*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "veX-I4ZP-Xjv"
   },
   "source": [
    "# **Save the  results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAIlTgKwyjS6",
    "outputId": "dd61db00-7635-41e9-f43d-ef2d1859fba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/models/CodeGeneration/MarianCG-CoNaLa-Large\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "NS4mH-3KVZug",
    "outputId": "a7501097-b854-418e-b46c-bc4edc1ea69e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/models/CodeGeneration/New/DJANGO/distilRoBERTa-Marian'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wHa-z0Fy5z3"
   },
   "outputs": [],
   "source": [
    "bleu_score = 0\n",
    "with open(\"./distilroberta-marian-training/Output.txt\", 'w') as f:\n",
    "  for i in range(len(results)):\n",
    "    nl = results[\"nl\"][i]\n",
    "    ref = results[\"code\"][i]\n",
    "    pred = results[\"pred_code\"][i]\n",
    "\n",
    "    if pred is not None and pred != \"\":\n",
    "      if ref is not None and ref != \"\":\n",
    "\n",
    "        bleu_metric = evaluator.evaluate([pred], [ref])\n",
    "        f.write( \"NL: \" + nl + '\\n')\n",
    "        f.write( \"ref: \" + ref + '\\n')\n",
    "        f.write( \"Output: \" + pred + '\\n')\n",
    "        f.write(\"bleu_metric: \" + str(bleu_metric[\"BLEU\"]) + '\\n')\n",
    "        f.write(10*\"-\" + '\\n')\n",
    "        bleu_score  += bleu_metric[\"BLEU\"]\n",
    "\n",
    "    else:\n",
    "      continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCTiVafqjEc5"
   },
   "source": [
    "# **Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NcbWMi7ejEc5"
   },
   "outputs": [],
   "source": [
    "# map data correctly\n",
    "def generate_code(NL):\n",
    "    inputs = new_tokenizer([NL], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask.to(\"cuda\")\n",
    "\n",
    "    outputs = new_model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_code = new_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    return output_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ymh6pfPjEc6"
   },
   "outputs": [],
   "source": [
    "\n",
    "#NL = \"decode a hex string to utf-8.\"\n",
    "NL = \"remove first and last lines of string `s`\"\n",
    "output_code = generate_code(NL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mMHqDePjjEc7"
   },
   "outputs": [],
   "source": [
    "output_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIP7fiupjEc7"
   },
   "outputs": [],
   "source": [
    "natural_language = \"How I can get rid of None values in dictionary?\"\n",
    "gen_code = generate_code(natural_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QhjxswQrjEc8"
   },
   "outputs": [],
   "source": [
    "gen_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_QuAYeQjDkF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGxnFOE2jDgB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bhfGalXdjDb-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gS48lsC8jDWQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfBCBRRVjDPL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0284945412944d3bb3901feded769c1b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0339c2dfbe10438b819e9d3d738a744c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04eaec24117b4323b48d2276298e0369": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "053137ce2f694b598b75fc54dc56d27f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "065bdc3d1e7d4570939a940a759c6f90": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "07229f5fc731423391813864a18b5cb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "0890cd7ca192432caf74e2e9074dc9c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0941dec0d338463bbcf6d27d281a394b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e56973a6fa7140c0a6fc0654b9dc7e6c",
      "placeholder": "​",
      "style": "IPY_MODEL_ff58a8fb8a354b73b165b739d0f7fac8",
      "value": " 1805/1805 [03:04&lt;00:00,  9.99ba/s]"
     }
    },
    "09a654952538409fa1d8e669045d4057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d4f8711dba3482fa4aa7bf6c4e0dec3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "103adab7cccc4dfdaffac04933db7a01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1085062f824e44ed80b2f35714a56b01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13d5ba2a4856419faadc38f2eb9f6a26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14aea39ed06b4540b3c4e19321fdb143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1576756311624e76b0d0e9aea9044f18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "160d590b0a4d44a99cb75731d29d787b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1677fd9f88e948b08e7a7f770c73abab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "18ef2ec0220c419ea51b49879a0522a3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1e122b742b3c4503987042041b812deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec5c336b78fd45e5b1f9ccb266d6e6ae",
      "max": 1805,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_29f565a53b7b4c639350d442d40bcf5c",
      "value": 1805
     }
    },
    "1e3287543fec49f281bff052e7d2e07d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_636bf065ab50407abee10ec045c4f816",
      "placeholder": "​",
      "style": "IPY_MODEL_d8ea81ed0a614ff3a7030cb1a0ff1035",
      "value": " 4.07k/? [00:00&lt;00:00, 125kB/s]"
     }
    },
    "1fd5d91eeb3945f4bf538d5858e41745": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "235035c3d52f4dd0bfce44433f519308": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0284945412944d3bb3901feded769c1b",
      "max": 117007,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_18ef2ec0220c419ea51b49879a0522a3",
      "value": 117007
     }
    },
    "276e046c19054cc794e4afcbfb3bab87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "27b8c868bd9344e48469f6d19de59509": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "287bee6cc9fa40639d9843786ec56c62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbba476f424c40038312d723a5e8c77d",
      "placeholder": "​",
      "style": "IPY_MODEL_4a577ef0459d4d9cafc17b5d82aaa9fb",
      "value": "100%"
     }
    },
    "29f565a53b7b4c639350d442d40bcf5c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d6d56af44d5468ca4cd89e523c0d789": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b096693119604256bd03a10f2f903fc6",
       "IPY_MODEL_c541e34d137f4ddbbe4941b8f3031698",
       "IPY_MODEL_6d3d8cae0ebb442eb9e123c028b70590"
      ],
      "layout": "IPY_MODEL_3e3b43d83fe648bd812b0887501cbcb5"
     }
    },
    "316ff94579fe48c49248f7767b1a3cb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "34fc3354312c44f9b93eed319498f2a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "366b6e8cbbd74087acd6e4a08e844249": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38dbb389ae3849818d169823dcdf4b94": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3aae563b52334d28a470a4135feeaedd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_545e4ca96cac463ebffc834b0cc9e39b",
      "max": 1000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ea48b58f54054b59b7287adcc87f1f58",
      "value": 1000
     }
    },
    "3e3b43d83fe648bd812b0887501cbcb5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ef72bad1fba426a9e9996d719db27aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_04eaec24117b4323b48d2276298e0369",
      "placeholder": "​",
      "style": "IPY_MODEL_5e3c35e9e79248f69fc5ecf87099088b",
      "value": " 16000/16000 [00:00&lt;00:00, 96083.06 examples/s]"
     }
    },
    "3f0065aa04f14fbdb7a9b0c47c3dd9c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_99b99ae0eae3448583131778d153461b",
       "IPY_MODEL_1e122b742b3c4503987042041b812deb",
       "IPY_MODEL_fedc2b818cfe475baa77fc0f0446ba63"
      ],
      "layout": "IPY_MODEL_c0debc0a85114e5e8a33003e66940abb"
     }
    },
    "3f4c22f7191d48ebadcb98a8a62b4ad9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41040247b4524eeb810b6e58ad5d9e1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "421153cdc9d44147ab2b43d1f9ec4cd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9e7f60a229a740baaa39af04b118c878",
      "placeholder": "​",
      "style": "IPY_MODEL_103adab7cccc4dfdaffac04933db7a01",
      "value": "Downloading data: 100%"
     }
    },
    "47a44799fd9946bc87ffc057ce725437": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "489de527d51a45e49943d5fedc9eb7e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d5cde4bea5234d158e8cfe63ef3175ac",
      "placeholder": "​",
      "style": "IPY_MODEL_47a44799fd9946bc87ffc057ce725437",
      "value": " 7.65k/? [00:00&lt;00:00, 207kB/s]"
     }
    },
    "4a577ef0459d4d9cafc17b5d82aaa9fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4e12d866d32d414ca37a794207e26158": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5047b33728a645478f8037e8c01c4fcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8bc8b76f33f5451d9fd74769cba111f4",
      "placeholder": "​",
      "style": "IPY_MODEL_b8d94bdeb025478cb6456ee717fc9349",
      "value": "Downloading data: 100%"
     }
    },
    "545e4ca96cac463ebffc834b0cc9e39b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "548522de6b11407d898ea2fbc41f25ed": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "566bd4e830fa404fb1cc613a4610b285": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5a328a1e71df4a9ab21142885a3ee277": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5a67d10adf25484097cd063c9ada7439": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a2a8e36358e74f6abc3f2e80e7c77e17",
       "IPY_MODEL_903c88afd5a648308d7847d8f8a29447",
       "IPY_MODEL_81249daf5ccd41558c41afedd07a3d71"
      ],
      "layout": "IPY_MODEL_e1304c686ac849c4aaeb4f4f3ee653da"
     }
    },
    "5af8b7b00b9a45d598eb37886c0dcfe8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f3f1594da1e648208a83f301e1294b40",
       "IPY_MODEL_cfa92722550c40c4adad981f263dedf0",
       "IPY_MODEL_dc94c2c9199444dfa552740e72dbcae5"
      ],
      "layout": "IPY_MODEL_09a654952538409fa1d8e669045d4057"
     }
    },
    "5c29a4db804e4785a30ab9ad3d15b598": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5e3c35e9e79248f69fc5ecf87099088b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5f34c9b3366a4eb6ba1addd1d0137353": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "61a9abd5145b4599a94b7eb0740c7f62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ce63e9942734429f9e1730a6f00a5643",
       "IPY_MODEL_a46278423c0146b7a43baba00bd24a62",
       "IPY_MODEL_1e3287543fec49f281bff052e7d2e07d"
      ],
      "layout": "IPY_MODEL_7a2edc562b0a47c49608061ada9134b4"
     }
    },
    "62b445053fd84f3daf4547b9f6dec98d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "636bf065ab50407abee10ec045c4f816": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65134a0abb034b31a41f5c9c8cddf233": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0d4f8711dba3482fa4aa7bf6c4e0dec3",
      "placeholder": "​",
      "style": "IPY_MODEL_9b4f6510e65144e5b91f4a59a95008c6",
      "value": " 117k/117k [00:00&lt;00:00, 267kB/s]"
     }
    },
    "679ec853bb454f349d13f5ef8ad9e1ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d165d1957ea42219deaa6c277a4f832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6d3d8cae0ebb442eb9e123c028b70590": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_27b8c868bd9344e48469f6d19de59509",
      "placeholder": "​",
      "style": "IPY_MODEL_9722e321e6c142a6a65212ba3689c821",
      "value": " 5.67k/5.67k [00:00&lt;00:00, 170kB/s]"
     }
    },
    "7139605f3b2a409890859b0a9bfe976d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75cee6cc1f444045bf227545fdaddacf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "769be8f1dadf44cab5771d334814b68b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f4c22f7191d48ebadcb98a8a62b4ad9",
      "max": 2848,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_316ff94579fe48c49248f7767b1a3cb2",
      "value": 2848
     }
    },
    "76b8f1175b4043a6be744fbe955bcbdd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "771cf0b512014fada9647213fb822c1d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1be9004748b4e99b191362efeba9390",
      "placeholder": "​",
      "style": "IPY_MODEL_5f34c9b3366a4eb6ba1addd1d0137353",
      "value": "Downloading builder script: "
     }
    },
    "77d2047f044145d7a8587926852b3c2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c6ed4640d99b4159aded9f38d5d54223",
      "max": 1913855,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9ac961f84eee4c898795a0f0681f5451",
      "value": 1913855
     }
    },
    "78da3bdad2f34aa985d4af72db0f7ea7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_771cf0b512014fada9647213fb822c1d",
       "IPY_MODEL_84cc57f64e584de094843099c8d41384",
       "IPY_MODEL_a4ddbde4e1e1476d9f80237da102ee6a"
      ],
      "layout": "IPY_MODEL_a7c900de40a54173aaf6f45949f8b6f3"
     }
    },
    "7a2edc562b0a47c49608061ada9134b4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d05d1de0e3045cc80fb7c227ca26c59": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a8d4a065b23f49b49658ab5549975cab",
       "IPY_MODEL_ce7d97731a1c417c80a23a5a3baadf25",
       "IPY_MODEL_f4621dc76cec4719b17cd28affb6bea2"
      ],
      "layout": "IPY_MODEL_e8d5fc93a0414eb2a72f5acf63907d0f"
     }
    },
    "81249daf5ccd41558c41afedd07a3d71": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a84d8126ce54400e99042d9453cf3a02",
      "placeholder": "​",
      "style": "IPY_MODEL_7139605f3b2a409890859b0a9bfe976d",
      "value": " 5.60k/? [00:00&lt;00:00, 168kB/s]"
     }
    },
    "814a0d83738248d09783a4a3a095955f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "84cc57f64e584de094843099c8d41384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_053137ce2f694b598b75fc54dc56d27f",
      "max": 2478,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d023a09a50094505bf752d7e42ab1532",
      "value": 2478
     }
    },
    "8514b667f88c44a5bdf7c7d96af5c8a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85871408bd954366bd203876c789f1ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e3be7c15a27b4d1297209bbb5c35dbf7",
      "placeholder": "​",
      "style": "IPY_MODEL_b5cc25381bfa4e7c9722e334553fffa0",
      "value": "Downloading data: 100%"
     }
    },
    "8725dcdf1b804c6a93d78c6153b166fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f8ba7da989374ba48c819d0931d7409a",
       "IPY_MODEL_769be8f1dadf44cab5771d334814b68b",
       "IPY_MODEL_489de527d51a45e49943d5fedc9eb7e9"
      ],
      "layout": "IPY_MODEL_1677fd9f88e948b08e7a7f770c73abab"
     }
    },
    "89a2c7f83bcb4afdb2498be0a95b3aa8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bc8b76f33f5451d9fd74769cba111f4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d5e5ab2dcf6472eb9dc26395f8bdc64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_421153cdc9d44147ab2b43d1f9ec4cd3",
       "IPY_MODEL_235035c3d52f4dd0bfce44433f519308",
       "IPY_MODEL_65134a0abb034b31a41f5c9c8cddf233"
      ],
      "layout": "IPY_MODEL_75cee6cc1f444045bf227545fdaddacf"
     }
    },
    "8d857025207e4a8e82010fbeb2da51d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8fb6a63acfe34842b2218b070af450bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fd5d91eeb3945f4bf538d5858e41745",
      "max": 226086,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_955fd950678742a3907e5e65e59fd2b9",
      "value": 226086
     }
    },
    "903c88afd5a648308d7847d8f8a29447": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f655aa47ff0245749d9104d3cd4b1413",
      "max": 2160,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8d857025207e4a8e82010fbeb2da51d7",
      "value": 2160
     }
    },
    "9409df9ac83f440fa0f6b6aad3dd9876": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f39c1c3ac41240709307bc6f1585792f",
       "IPY_MODEL_bd9c04a07c3945569774607e902b76a0",
       "IPY_MODEL_3ef72bad1fba426a9e9996d719db27aa"
      ],
      "layout": "IPY_MODEL_dc815f2b6915436aa66dccd8a01965cd"
     }
    },
    "955fd950678742a3907e5e65e59fd2b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9722e321e6c142a6a65212ba3689c821": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "973ad2e166c44f1383c86e47bf423c69": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "99b99ae0eae3448583131778d153461b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_548522de6b11407d898ea2fbc41f25ed",
      "placeholder": "​",
      "style": "IPY_MODEL_0890cd7ca192432caf74e2e9074dc9c5",
      "value": "Generating test split: 100%"
     }
    },
    "9ac961f84eee4c898795a0f0681f5451": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9b4f6510e65144e5b91f4a59a95008c6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e7f60a229a740baaa39af04b118c878": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1ae6f0447444e2b97d16cf324c2fd70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a1be9004748b4e99b191362efeba9390": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a2a8e36358e74f6abc3f2e80e7c77e17": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d179d33897a34382904fbb9ade5443bf",
      "placeholder": "​",
      "style": "IPY_MODEL_a6d3d7ae41c6420992d10ed2689711dc",
      "value": "Downloading builder script: "
     }
    },
    "a35d287ba2f34078901da04b4669e59f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a46278423c0146b7a43baba00bd24a62": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8514b667f88c44a5bdf7c7d96af5c8a2",
      "max": 1554,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_07229f5fc731423391813864a18b5cb3",
      "value": 1554
     }
    },
    "a4ddbde4e1e1476d9f80237da102ee6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_276e046c19054cc794e4afcbfb3bab87",
      "placeholder": "​",
      "style": "IPY_MODEL_d4d31c5cc018487d8ec84a8ecde10de8",
      "value": " 6.06k/? [00:00&lt;00:00, 194kB/s]"
     }
    },
    "a6d3d7ae41c6420992d10ed2689711dc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7c900de40a54173aaf6f45949f8b6f3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a84d8126ce54400e99042d9453cf3a02": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8d4a065b23f49b49658ab5549975cab": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8329901fac044609984bea6ce1b433e",
      "placeholder": "​",
      "style": "IPY_MODEL_b431df0658bf4bb394db58f273ed1d70",
      "value": "Map: 100%"
     }
    },
    "ae2066ba514d424caf1673f460f62068": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f059a3ac70e543af87ea70a19a4ce593",
       "IPY_MODEL_3aae563b52334d28a470a4135feeaedd",
       "IPY_MODEL_f228afa8d80f484fb0b5cdc50add038b"
      ],
      "layout": "IPY_MODEL_76b8f1175b4043a6be744fbe955bcbdd"
     }
    },
    "aec2f7dbbe654edf898c2fae73831d0d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b096693119604256bd03a10f2f903fc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_814a0d83738248d09783a4a3a095955f",
      "placeholder": "​",
      "style": "IPY_MODEL_6d165d1957ea42219deaa6c277a4f832",
      "value": "Downloading builder script: 100%"
     }
    },
    "b431df0658bf4bb394db58f273ed1d70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b5cc25381bfa4e7c9722e334553fffa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b80b9133ac9841c6bf87031a6c5c5adb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b81a4aef890a42b2b9b324983dd03ad1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b8d94bdeb025478cb6456ee717fc9349": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbbb3939329648199334555b96d16043": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bbf4adc7b5c046d5a46e5c05bb4595a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bbf64fac4d5044bda43abd182da2b8f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "bd9c04a07c3945569774607e902b76a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a35d287ba2f34078901da04b4669e59f",
      "max": 16000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1085062f824e44ed80b2f35714a56b01",
      "value": 16000
     }
    },
    "be52f6d00b364349b798ee6ca28995d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c01732c6e7e949bcaa75e5d2b8677a63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be52f6d00b364349b798ee6ca28995d0",
      "max": 1805,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_bbf64fac4d5044bda43abd182da2b8f6",
      "value": 1805
     }
    },
    "c0debc0a85114e5e8a33003e66940abb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4e25c26834b41918918544ba881bc89": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c541e34d137f4ddbbe4941b8f3031698": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1576756311624e76b0d0e9aea9044f18",
      "max": 5669,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5a328a1e71df4a9ab21142885a3ee277",
      "value": 5669
     }
    },
    "c6ed4640d99b4159aded9f38d5d54223": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce3cac922be741e2bb3700fbdb737cfb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce63e9942734429f9e1730a6f00a5643": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5b04f0c0b1141798f82671c4bcc7f1a",
      "placeholder": "​",
      "style": "IPY_MODEL_bbf4adc7b5c046d5a46e5c05bb4595a8",
      "value": "Downloading extra modules: "
     }
    },
    "ce7d97731a1c417c80a23a5a3baadf25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aec2f7dbbe654edf898c2fae73831d0d",
      "max": 166,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5c29a4db804e4785a30ab9ad3d15b598",
      "value": 166
     }
    },
    "ceae1bd576a8457088a01c0f89d75a05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfa92722550c40c4adad981f263dedf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ceae1bd576a8457088a01c0f89d75a05",
      "max": 1240,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14aea39ed06b4540b3c4e19321fdb143",
      "value": 1240
     }
    },
    "d023a09a50094505bf752d7e42ab1532": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d179d33897a34382904fbb9ade5443bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d3fd2d06408a42c7b841779b03197d6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4d31c5cc018487d8ec84a8ecde10de8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d5cde4bea5234d158e8cfe63ef3175ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8ea81ed0a614ff3a7030cb1a0ff1035": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "da8e78ccb4614ede950928f5d57835f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5047b33728a645478f8037e8c01c4fcf",
       "IPY_MODEL_8fb6a63acfe34842b2218b070af450bd",
       "IPY_MODEL_fadc4c8e8d3b46b2b5863f579c326538"
      ],
      "layout": "IPY_MODEL_0339c2dfbe10438b819e9d3d738a744c"
     }
    },
    "dbba476f424c40038312d723a5e8c77d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc815f2b6915436aa66dccd8a01965cd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc94c2c9199444dfa552740e72dbcae5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13d5ba2a4856419faadc38f2eb9f6a26",
      "placeholder": "​",
      "style": "IPY_MODEL_dd0dee67fbfa47faa897ab344808fc87",
      "value": " 1.24k/1.24k [00:00&lt;00:00, 81.3kB/s]"
     }
    },
    "dd0dee67fbfa47faa897ab344808fc87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e1304c686ac849c4aaeb4f4f3ee653da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e2958a5a23974fad878c699b664d5e42": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_85871408bd954366bd203876c789f1ec",
       "IPY_MODEL_77d2047f044145d7a8587926852b3c2f",
       "IPY_MODEL_f352f8ec0d7e4ff0ad2e9673442b31d3"
      ],
      "layout": "IPY_MODEL_b80b9133ac9841c6bf87031a6c5c5adb"
     }
    },
    "e3be7c15a27b4d1297209bbb5c35dbf7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e56973a6fa7140c0a6fc0654b9dc7e6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5b04f0c0b1141798f82671c4bcc7f1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e8d5fc93a0414eb2a72f5acf63907d0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea48b58f54054b59b7287adcc87f1f58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec5c336b78fd45e5b1f9ccb266d6e6ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ecd2b56bd1d649cfb0183b4665eb36d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_287bee6cc9fa40639d9843786ec56c62",
       "IPY_MODEL_c01732c6e7e949bcaa75e5d2b8677a63",
       "IPY_MODEL_0941dec0d338463bbcf6d27d281a394b"
      ],
      "layout": "IPY_MODEL_4e12d866d32d414ca37a794207e26158"
     }
    },
    "f059a3ac70e543af87ea70a19a4ce593": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_38dbb389ae3849818d169823dcdf4b94",
      "placeholder": "​",
      "style": "IPY_MODEL_065bdc3d1e7d4570939a940a759c6f90",
      "value": "Generating validation split: 100%"
     }
    },
    "f228afa8d80f484fb0b5cdc50add038b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c4e25c26834b41918918544ba881bc89",
      "placeholder": "​",
      "style": "IPY_MODEL_b81a4aef890a42b2b9b324983dd03ad1",
      "value": " 1000/1000 [00:00&lt;00:00, 24983.05 examples/s]"
     }
    },
    "f352f8ec0d7e4ff0ad2e9673442b31d3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce3cac922be741e2bb3700fbdb737cfb",
      "placeholder": "​",
      "style": "IPY_MODEL_34fc3354312c44f9b93eed319498f2a1",
      "value": " 1.91M/1.91M [00:01&lt;00:00, 1.74MB/s]"
     }
    },
    "f39c1c3ac41240709307bc6f1585792f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3fd2d06408a42c7b841779b03197d6f",
      "placeholder": "​",
      "style": "IPY_MODEL_973ad2e166c44f1383c86e47bf423c69",
      "value": "Generating train split: 100%"
     }
    },
    "f3d1382e735d4660baa87509a447b2d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f3f1594da1e648208a83f301e1294b40": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89a2c7f83bcb4afdb2498be0a95b3aa8",
      "placeholder": "​",
      "style": "IPY_MODEL_566bd4e830fa404fb1cc613a4610b285",
      "value": "Downloading readme: 100%"
     }
    },
    "f4621dc76cec4719b17cd28affb6bea2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_366b6e8cbbd74087acd6e4a08e844249",
      "placeholder": "​",
      "style": "IPY_MODEL_f3d1382e735d4660baa87509a447b2d6",
      "value": " 166/166 [00:00&lt;00:00, 510.61 examples/s]"
     }
    },
    "f655aa47ff0245749d9104d3cd4b1413": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8329901fac044609984bea6ce1b433e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8ba7da989374ba48c819d0931d7409a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_160d590b0a4d44a99cb75731d29d787b",
      "placeholder": "​",
      "style": "IPY_MODEL_62b445053fd84f3daf4547b9f6dec98d",
      "value": "Downloading builder script: "
     }
    },
    "fadc4c8e8d3b46b2b5863f579c326538": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bbbb3939329648199334555b96d16043",
      "placeholder": "​",
      "style": "IPY_MODEL_41040247b4524eeb810b6e58ad5d9e1f",
      "value": " 226k/226k [00:00&lt;00:00, 298kB/s]"
     }
    },
    "fedc2b818cfe475baa77fc0f0446ba63": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_679ec853bb454f349d13f5ef8ad9e1ce",
      "placeholder": "​",
      "style": "IPY_MODEL_a1ae6f0447444e2b97d16cf324c2fd70",
      "value": " 1805/1805 [00:00&lt;00:00, 46973.50 examples/s]"
     }
    },
    "ff58a8fb8a354b73b165b739d0f7fac8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
